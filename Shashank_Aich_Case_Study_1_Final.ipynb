{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Telephonic Marketing Outcome for Bank Term Deposit Final Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Utility-Functions-for-ML-Pipeline\" data-toc-modified-id=\"Utility-Functions-for-ML-Pipeline-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Utility Functions for ML Pipeline</a></span></li><li><span><a href=\"#Final-call-to-ML-pipeline\" data-toc-modified-id=\"Final-call-to-ML-pipeline-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Final call to ML pipeline</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T11:12:47.541437Z",
     "start_time": "2020-04-02T11:12:47.535924Z"
    }
   },
   "outputs": [],
   "source": [
    "#function that will return the sine of the feature\n",
    "def return_sin(x):\n",
    "    return(np.sin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions will convert the numerical data to categorical data\n",
    "#pdays, cons.price.idx, cons.conf.idx, nr.employee\n",
    "\n",
    "\n",
    "#convert pdays to pdays_cat\n",
    "def convert_pdays_cat(x):\n",
    "    #threshold=512.5 if t<=512.5 then pdays_cat_yes else pdays_cat_no\n",
    "    x['pdays_cat'] = np.where(x['pdays'] <= 513, 'pdays_cat_yes',\n",
    "                              'pdays_cat_no')\n",
    "\n",
    "\n",
    "#convert cons.price.idx to cons.price.idx_cat\n",
    "def convert_cons_price_idx_cat(x):\n",
    "    #threshold=92.868 if t<=92.868 then cons_price_idx_cat_yes else cons_price_idx_cat_no\n",
    "    x['cons.price.idx_cat'] = np.where(x['cons.price.idx'] <= 92.868,\n",
    "                                       'cons_price_idx_yes',\n",
    "                                       'cons_price_idx_no')\n",
    "\n",
    "\n",
    "#convert cons.conf.idx to cons.conf.idx_cat\n",
    "def convert_cons_conf_idx_cat(x):\n",
    "    #threshold = -35.45 if t<=-35.45 then cons_conf_idx_cat_no else cons_conf_idx_cat_yes\n",
    "    x['cons.conf.idx_cat'] = np.where(x['cons.conf.idx'] <= -35.45,\n",
    "                                      'cons_conf_idx_cat_no',\n",
    "                                      'cons_conf_idx_cat_yes')\n",
    "\n",
    "\n",
    "#convert nr.employed to nr.employed_cat\n",
    "def convert_nr_employed_cat(x):\n",
    "    #threshold=5087.65 if t<=5087.65 then nr_employed_cat_yes else nr_employed_cat_no\n",
    "    x['nr.employed_cat'] = np.where(x['nr.employed'] <= 5087.65,\n",
    "                                    'nr_employed_cat_yes',\n",
    "                                    'nr_employed_cat_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given a dataframe this function will perform data preprocessing\n",
    "def data_preprocessing(data):\n",
    "    #some col in the dataset have values as unknown we will change it to unknown_col_name\n",
    "\n",
    "    #Replace the unknown job level\n",
    "    data['job'].replace('unknown', 'unknown_job', inplace=True)\n",
    "\n",
    "    #Replace the unknown marital status\n",
    "    data['marital'].replace('unknown', 'unknown_marital', inplace=True)\n",
    "\n",
    "    #Replace the unknown education level\n",
    "    data['education'].replace('unknown', 'unknown_education', inplace=True)\n",
    "\n",
    "    #Replace the unknown default level\n",
    "    data['default'].replace('unknown', 'unknown_default', inplace=True)\n",
    "\n",
    "    #Replace the unknown housing level\n",
    "    data['housing'].replace('unknown', 'unknown_housing', inplace=True)\n",
    "\n",
    "    #Replace the unknown loan level\n",
    "    data['loan'].replace('unknown', 'unknown_loan', inplace=True)\n",
    "\n",
    "    #Replace the unknown and contact level\n",
    "    data['contact'].replace('unknown', 'unknown_contact', inplace=True)\n",
    "\n",
    "    #Replace the unknown and month level\n",
    "    data['month'].replace('unknown', 'unknown_month', inplace=True)\n",
    "\n",
    "    #Replace the unknown and day_of_week level\n",
    "    data['day_of_week'].replace('unknown', 'unknown_day_of_week', inplace=True)\n",
    "\n",
    "    #Replace the unknown and poutcome level\n",
    "    data['poutcome'].replace('unknown', 'unknown_poutcome', inplace=True)\n",
    "\n",
    "    #removing 'hiphen' and 'dot' from the category names\n",
    "    #cleaning job data\n",
    "    z = 'job'\n",
    "    data['job'].replace('blue-collar', 'blue_collar', inplace=True)\n",
    "    data['job'].replace('self-employed', 'self_employed', inplace=True)\n",
    "\n",
    "    #cleaning education data\n",
    "    data['education'].replace('university.degree',\n",
    "                              'university_degree',\n",
    "                              inplace=True)\n",
    "    data['education'].replace('high.school', 'high_school', inplace=True)\n",
    "    data['education'].replace('basic.9y', 'basic_9y', inplace=True)\n",
    "    data['education'].replace('professional.course',\n",
    "                              'professional_course',\n",
    "                              inplace=True)\n",
    "    data['education'].replace('basic.4y', 'basic_4y', inplace=True)\n",
    "    data['education'].replace('basic.6y', 'basic_6y', inplace=True)\n",
    "\n",
    "    #cleaning default data\n",
    "    #renaming 'yes' and 'no' in the default names\n",
    "    data['default'].replace('yes', 'yes_default', inplace=True)\n",
    "    data['default'].replace('no', 'no_default', inplace=True)\n",
    "\n",
    "    #cleaning housing data\n",
    "    #renaming 'yes' and 'no' in the housing names\n",
    "    data['housing'].replace('yes', 'yes_housing', inplace=True)\n",
    "    data['housing'].replace('no', 'no_housing', inplace=True)\n",
    "\n",
    "    #cleaning loan data\n",
    "    #renaming 'yes' and 'no' in the loan names\n",
    "    data['loan'].replace('yes', 'yes_loan', inplace=True)\n",
    "    data['loan'].replace('no', 'no_loan', inplace=True)\n",
    "\n",
    "    #cleaning poutcome data\n",
    "    #renaming 'nonexistent', 'failure', 'success' in the poutcome names\n",
    "    data['poutcome'].replace('nonexistent',\n",
    "                             'nonexistent_poutcome',\n",
    "                             inplace=True)\n",
    "    data['poutcome'].replace('failure', 'failure_poutcome', inplace=True)\n",
    "    data['poutcome'].replace('success', 'success_poutcome', inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will add the engineered features to the data\n",
    "def add_engineered_featurs(data):\n",
    "    #add the categorical features for the cols pdays, cons_price_idx, cons_conf_idx, nr_employed\n",
    "    convert_pdays_cat(data)\n",
    "    convert_cons_price_idx_cat(data)\n",
    "    convert_cons_conf_idx_cat(data)\n",
    "    convert_nr_employed_cat(data)\n",
    "    \n",
    "    #add sine features to numerical data\n",
    "    cols_num = [\n",
    "    'age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate',\n",
    "    'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'\n",
    "    ]\n",
    "    \n",
    "    #add sine features for numeral data\n",
    "    for col in cols_num:\n",
    "        z = col\n",
    "        z_sin = col + '_sin'\n",
    "        #get sine of feature and add it to the dataframe\n",
    "        tmp_sin = return_sin(data[z].values)\n",
    "        data[z_sin] = tmp_sin\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will standardize the data given a dataframe\n",
    "#if the data is train data we will have to call to the function with tag = 1\n",
    "#if the data is test/cv we will have to call the function with tag = 0\n",
    "#The function will return the the np array after standardization process\n",
    "#The standard_dict_cat is a dictionary of categorical feature names and their corresponding countvectorizer objects\n",
    "#The standard_dict_num is a dictionary of numerical feature names and their corresponding standardscalar objects\n",
    "#if the training data is provided the function will add values to the dectionaries standard_dict_cat,standard_dict_num\n",
    "# Initially both the dectionaries will be initialized as <'feature_name':0>\n",
    "# The value will be filled in the train process\n",
    "# The same dictionaries will be used with the test/cv to transform data\n",
    "\n",
    "\n",
    "def standardize_data(dataframe, tag, standard_dict_cat, standard_dict_num):\n",
    "    #if the training data is passed with 0 initialized dectionaries\n",
    "    if tag == 1:\n",
    "        #Initialize the feature list\n",
    "        features = []\n",
    "        #Initialize the array for hstacking in the end\n",
    "        array = []\n",
    "        #convert categorical features to one hot encoding using countvectorizer\n",
    "        for cat in standard_dict_cat.keys():\n",
    "            #initialize the vectorizer\n",
    "            vectorizer = CountVectorizer()\n",
    "            #fit and transform train data\n",
    "            X = vectorizer.fit_transform(dataframe[cat])\n",
    "            #get feature names\n",
    "            Z = vectorizer.get_feature_names()\n",
    "            #add vectorizer to the dectionary\n",
    "            standard_dict_cat[cat] = vectorizer\n",
    "            #add features to the feature array\n",
    "            features.extend(Z)\n",
    "            #add data to the np array\n",
    "            array.append(X)\n",
    "        #scale numerical features using standardscalar\n",
    "        for num in standard_dict_num.keys():\n",
    "            #initialize the standardscalar\n",
    "            scaler = StandardScaler()\n",
    "            #fit and transform train data\n",
    "            X = scaler.fit_transform(dataframe[num].values.reshape(-1, 1))\n",
    "            #add scalar to the dectionary\n",
    "            standard_dict_num[num] = scaler\n",
    "            #add features to the feature array\n",
    "            features.append(num)\n",
    "            #add data to the np array\n",
    "            array.append(X)\n",
    "\n",
    "        #preparing np array for returing\n",
    "        data_stand = sparse.hstack(\n",
    "            (array[0], array[1], array[2], array[3], array[4], array[5],\n",
    "             array[6], array[7], array[8], array[9], array[10], array[11],\n",
    "             array[12], array[13], array[14], array[15], array[16], array[17],\n",
    "             array[18], array[19], array[20], array[21], array[22], array[23],\n",
    "             array[24], array[25], array[26], array[27], array[28], array[29],\n",
    "             array[30], array[31], array[32], array[33]))\n",
    "        #return data and the features\n",
    "        return data_stand, features\n",
    "    if tag == 0:\n",
    "        #Initialize the feature list\n",
    "        features = []\n",
    "        #Initialize the array for hstacking in the end\n",
    "        array = []\n",
    "        #convert categorical features to one hot encoding using countvectorizer\n",
    "        for cat in standard_dict_cat.keys():\n",
    "            #transform train data\n",
    "            X = standard_dict_cat[cat].transform(dataframe[cat])\n",
    "            #get feature names\n",
    "            Z = standard_dict_cat[cat].get_feature_names()\n",
    "            #add features to the feature array\n",
    "            features.extend(Z)\n",
    "            #add data to the np array\n",
    "            array.append(X)\n",
    "        #scale numerical features using standardscalar\n",
    "        for num in standard_dict_num.keys():\n",
    "            #transform train data\n",
    "            X = standard_dict_num[num].transform(dataframe[num].values.reshape(\n",
    "                -1, 1))\n",
    "            #add features to the feature array\n",
    "            features.append(num)\n",
    "            #add data to the np array\n",
    "            array.append(X)\n",
    "\n",
    "        #preparing np array for returing\n",
    "        data_stand = sparse.hstack(\n",
    "            (array[0], array[1], array[2], array[3], array[4], array[5],\n",
    "             array[6], array[7], array[8], array[9], array[10], array[11],\n",
    "             array[12], array[13], array[14], array[15], array[16], array[17],\n",
    "             array[18], array[19], array[20], array[21], array[22], array[23],\n",
    "             array[24], array[25], array[26], array[27], array[28], array[29],\n",
    "             array[30], array[31], array[32], array[33]))\n",
    "\n",
    "        #return data and the features\n",
    "        return data_stand, features\n",
    "    else:\n",
    "        return 'Please input a valid tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will return the standardization dict \n",
    "def get_standardization_dict():\n",
    "    #read data\n",
    "    df = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "    #drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    y_label = df['y']  #get output variable\n",
    "    le = LabelEncoder()\n",
    "    output = le.fit_transform(\n",
    "        y_label)  #transform the output variable to 0/1 form\n",
    "    df['output'] = output\n",
    "    Y_True = df['output'].values.reshape(-1,1)\n",
    "    #remove the 'y' and 'output' columns\n",
    "    df.drop(['y'], axis=1, inplace=True)\n",
    "    df.drop(['output'], axis=1, inplace=True)\n",
    "    #preprocessing data\n",
    "    data_preprocessing(df)\n",
    "    #add engineered features\n",
    "    add_engineered_featurs(df)\n",
    "    #standardizing the data\n",
    "    cols_cat = [\n",
    "        'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
    "        'month', 'day_of_week', 'poutcome', 'pdays_cat', 'cons.price.idx_cat',\n",
    "        'cons.conf.idx_cat', 'nr.employed_cat'\n",
    "    ]\n",
    "\n",
    "    #numerical features\n",
    "    cols_num = [\n",
    "        'age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate',\n",
    "        'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed',\n",
    "        'age_sin', 'duration_sin', 'campaign_sin', 'pdays_sin', 'previous_sin',\n",
    "        'emp.var.rate_sin', 'cons.price.idx_sin', 'cons.conf.idx_sin',\n",
    "        'euribor3m_sin', 'nr.employed_sin'\n",
    "    ]\n",
    "\n",
    "    #initializing the standard_dict_cat dectionary\n",
    "    standard_dict_cat = dict.fromkeys(cols_cat, 0)\n",
    "    #initializing the standard_dict_num dectionary\n",
    "    standard_dict_num = dict.fromkeys(cols_num, 0)\n",
    "\n",
    "    #here we will split our data randomly as there is no temporal nature of the data\n",
    "    df_Train_temp, df_Test, Y_Train_temp, Y_Test = train_test_split(df, Y_True, stratify=Y_True, test_size=0.3, random_state = 4) \n",
    "    df_Train, df_CV, Y_Train, Y_CV = train_test_split(df_Train_temp, Y_Train_temp, stratify=Y_Train_temp, test_size=0.3, random_state = 4)\n",
    "    #standardize the training data\n",
    "    X_Train, train_features = standardize_data(df_Train, 1, standard_dict_cat,\n",
    "                                               standard_dict_num)\n",
    "    #return the standardization dictionary for test data\n",
    "    return standard_dict_cat, standard_dict_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will read the ML Model from the disk \n",
    "def return_ml_model():\n",
    "    #lets load the Model from the disk\n",
    "    filename = 'finalized_model.sav'\n",
    "    # load the model from disk\n",
    "    model = pickle.load(open(filename, 'rb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the final function 1\n",
    "def final_function_1(data):\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    y_label = data['y']  #get output variable\n",
    "    le = LabelEncoder()\n",
    "    output = le.fit_transform(\n",
    "        y_label)  #transform the output variable to 0/1 form\n",
    "    data['output'] = output\n",
    "    #remove the 'y' and 'output' columns\n",
    "    data.drop(['y'], axis=1, inplace=True)\n",
    "    data.drop(['output'], axis=1, inplace=True)\n",
    "    #preprocessing data\n",
    "    data_preprocessing(data)\n",
    "    #add engineered features\n",
    "    add_engineered_featurs(data)\n",
    "    #get the standardization dict\n",
    "    standard_dict_cat, standard_dict_num = get_standardization_dict()\n",
    "    #standardizing the data\n",
    "    x_data, features = standardize_data(data, 0, standard_dict_cat,\n",
    "                                        standard_dict_num)\n",
    "    #get model\n",
    "    model = return_ml_model()\n",
    "    Y_Predicted = model.predict_proba(x_data)\n",
    "    Y_Predicted = Y_Predicted[:, -1]\n",
    "    y_Predicted = np.array(Y_Predicted > 0.5, dtype=int)\n",
    "    return y_Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the final function 2\n",
    "def final_function_2(data, y_data):\n",
    "    #import metric\n",
    "    from sklearn.metrics import f1_score\n",
    "    #drop duplicates\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    #preprocess data\n",
    "    data_preprocessing(data)\n",
    "    #add engineered features\n",
    "    add_engineered_featurs(data)\n",
    "    #get the standardization dict\n",
    "    standard_dict_cat, standard_dict_num = get_standardization_dict()\n",
    "    #standardizing the data\n",
    "    x_data, features = standardize_data(data, 0, standard_dict_cat,\n",
    "                                        standard_dict_num)\n",
    "    #get model\n",
    "    model = return_ml_model()\n",
    "    Y_Predicted = model.predict_proba(x_data)\n",
    "    Y_Predicted = Y_Predicted[:, -1]\n",
    "    y_Predicted = np.array(Y_Predicted > 0.5, dtype=int)\n",
    "    #get f1-score\n",
    "    f1_score = f1_score(y_data, y_Predicted, average='weighted')\n",
    "    return f1_score, y_Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final call to ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1-score is :  0.9623920265780731\n",
      "The f1-score is :  0.9623920265780731\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "\n",
    "#read data\n",
    "dff = pd.read_csv('bank-additional-full.csv', sep=';')\n",
    "#drop duplicates\n",
    "dff.drop_duplicates(inplace=True)\n",
    "#sample data from the df\n",
    "data = dff.sample(n=100)\n",
    "#get output variable\n",
    "y_label = data['y']\n",
    "le = LabelEncoder()\n",
    "#transform the output variable to 0/1 form\n",
    "output = le.fit_transform(y_label)\n",
    "data['output'] = output\n",
    "y_data = data['output'].values.reshape(-1, 1)\n",
    "#test final_fun_1\n",
    "y_Predicted = final_function_1(data)\n",
    "f1_score = f1_score(y_data, y_Predicted, average='weighted')\n",
    "print('The f1-score is : ', f1_score)\n",
    "\n",
    "#test final_fun_2\n",
    "f1_score, y_Predicted = final_function_2(data, y_data)\n",
    "print('The f1-score is : ', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook we Implemented the ML pipeline for the Bank Marketing Dataset\n",
    "- The Model that we used is the best Model from the experimented Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Applied AI Course\n",
    "- https://www.kaggle.com/nextbigwhat/eda-for-categorical-variables-a-beginner-s-way\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n",
    "- https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "- https://www.geeksforgeeks.org/permutation-and-combination-in-python/\n",
    "- https://seaborn.pydata.org/examples/distplot_options.html\n",
    "- https://stackoverflow.com/questions/14770735/how-do-i-change-the-figure-size-with-subplots\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "- https://towardsdatascience.com/how-to-visualize-a-decision-tree-in-5-steps-19781b28ffe2\n",
    "- https://towardsdatascience.com/visualizing-decision-trees-with-python-scikit-learn-graphviz-matplotlib-1c50b4aa68dc\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html\n",
    "- https://stackoverflow.com/questions/46659073/change-numerical-data-to-categorical-data-pandas\n",
    "- https://thispointer.com/python-how-to-use-if-else-elif-in-lambda-functions/\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html\n",
    "- https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe\n",
    "- https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "- https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf\n",
    "- https://www.kaggle.com/discdiver/guide-to-scaling-and-standardizing\n",
    "- https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n",
    "- https://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "- https://stackoverflow.com/questions/52410880/is-there-a-way-i-can-initialize-dictionary-values-to-0-in-python-taking-keys-fro\n",
    "- https://stackoverflow.com/questions/22257836/numpy-hstack-valueerror-all-the-input-arrays-must-have-same-number-of-dimens\n",
    "- https://stackoverflow.com/questions/26576524/how-do-i-transform-a-scipy-sparse-matrix-to-a-numpy-matrix\n",
    "- https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html\n",
    "- https://www.geeksforgeeks.org/append-extend-python/\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes\n",
    "- https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/Input\n",
    "- https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "- https://github.com/pranaysawant/Zomato-Restaurant-Rate-Prediction/blob/master/Zomato%20Restaurant%20Rating%20Prediction.ipynb\n",
    "- https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "- https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "- https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "- http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.classifier/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
